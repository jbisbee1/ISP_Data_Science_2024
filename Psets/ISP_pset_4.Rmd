---
title: "Problem Set 4"
author: "[YOUR NAME]"
date: "Due Date: 2024-07-19"
output:
  html_document: default
  pdf_document: default
subtitle: Regression Part 1
institute: Seoul National University
---

```{r,include=F}
knitr::opts_chunk$set(error=TRUE)
```


## Getting Set Up

Open `RStudio` and create a new RMarkDown file (`.Rmd`) by going to `File -> New File -> R Markdown...`.
Accept defaults and save this file as `[YOUR NAME]_ps4.Rmd` to your `code` folder.

Copy and paste the contents of this `.Rmd` file into your `[YOUR NAME]_ps4.Rmd` file. Then change the `author: [Your Name]` on line 2 to your name.

We will be using a new dataset called `youtube_individual.rds` which can be found on the course [github page](https://github.com/jbisbee1/ISP_Data_Science_2024/raw/main/data/youtube_individual.rds). The codebook for this dataset is produced below. All ideology measures are coded such that negative values indicate more liberal content and positive values indicate more conservative content.

| Name           | Description                                                                               |
|----------------|:------------------------------------------------------------------------------------------|
| ResponseId         | A unique code for each respondent to the survey   |
| average_recommendation_ideo   | The average ideology of all recommendations shown to the respondent             |
| average_current_ideo   | The average ideology of all current videos the respondent was watching when they were shown recommendations      |
| average_watch_ideo        | The average ideology of all videos the respondent has ever watched on YouTube |
| nReccs       | The total number of recommendations the respondent was shown during the survey                                    |
| YOB           | The year the respondent was born                                              |
| education     | The respondent's highest level of education                                  |
| gender   | The respondent's gender                               |
| income   | The respondent's total household income                               |
| party_id   | The respondent's self-reported partisanship                               |
| ideology   | The respondent's self-reported ideology                               |
| race   | The respondent's race                               |
| age   | The respondent's age at the time of the survey                               |

All of the following questions should be answered in this `.Rmd` file. There are code chunks with incomplete code that need to be filled in. 

This problem set is worth 10 total points, plus **two** extra credit questions, each worth **two** points. The point values for each question are indicated in brackets below. To receive full credit, you must have the correct code. In addition, some questions ask you to provide a written response in addition to the code.

You are free to rely on whatever resources you need to complete this problem set, including lecture notes, lecture presentations, Google, your classmates...you name it. However, the final submission must be complete by you. There are no group assignments. To submit, email the knitted output to Eun Ji Kim (kej990804@snu.ac.kr) **as a PDF** by the start of class on Friday, July 12th. If you need help converting to a PDF, see [this tutorial](https://github.com/jbisbee1/ISP_Data_Science_2024/blob/main/Psets/ISP_pset_0_HELPER.pdf).

**Good luck!**

*Copy the link to ChatGPT you used here: _________________


## Question 0
*Require `tidyverse` and load the `youtube_individual.rds` data to an object called `yt`.*

```{r}
require(tidyverse)
yt <- read_rds("https://github.com/jbisbee1/ISP_Data_Science_2024/raw/main/data/youtube_individual.rds")
```

## Question 1 [2 points]
*We are interested in how the YouTube recommendation algorithm works. These data are collected from real users, logged into their real YouTube accounts, allowing us to see who gets recommended which videos. We will investigate three research questions in this problem set:*

*1. What is the relationship between average ideology of recommendations shown to each user, and the average ideology of all the videos the user has watched?*

*2. What is the relationship between the average ideology of recommendations shown to each user, and the average ideology of the current video the user was watching when they were shown the recommendation?*

*3. Which of these relationships is stronger?*

*Start by answering all three of these research questions, and explaining your thinking.*

> Write answer here.

## Question 2 [1 point]

*Based on your previous answer, which variables are the $X$ (predictors) and which are the $Y$ (outcome) variables?*

> Write answer here

*Now create univariate visualizations of all three variables, making sure to label your plots clearly.*

```{r}
# INSERT CODE HERE
```

## Question 3 [1 point]

*Let's focus on the first research question. Create a multivariate visualization of the relationship between these two variables, making sure to put the $X$ variable on the x-axis, and the $Y$ variable on the y-axis. Add a straight line of best fit. Does the data support your theory?*

```{r}
# INSERT CODE HERE
```

> Write answer here.

## Question 4 [1 point]

*Now run a linear regression using the `lm()` function and save the result to an object called `model1`.*

```{r}
model1 <- lm(formula = ...,
             data = ...)
```

*Using either the `summary()` function (from base `R`) or the `tidy()` function (from the `broom` package), print the regression result.*

```{r}
# INSERT CODE HERE
```

*In a few sentences, summarize the results of the regression output. This requires you to translate the statistical measures into plain English, making sure to refer to the units for both the $X$ and $Y$ variables. In addition, you must determine whether the regression result supports your hypothesis, and discuss your confidence in your answer, referring to the p-value.*

> Write answer here.

## Question 5 [1 point]

*Now let's do the same thing for the second research question. First, create the multivariate visualization and determine whether it is consistent with your theory.*

```{r}
# INSERT CODE HERE
```

> Write answer here.

*Second, run a new regression and save the result to `model2`. Then print the result using either `summary()` or `tidy()`, as before.*

```{r}
# INSERT CODE HERE
```

*Finally, describe the result in plain English, and interpret it in light of your hypothesis. How confident are you?*

## EC #1 [2 points]

*Based* **ONLY** *on the preceding analysis, are you able to answer research question 3?*

## Question 7 [2 points]
*Now let's evaluate the models. Start by calculating the "mistakes" (i.e., the "errors" or the "residuals") generated by both models and saving these as new columns (`errors1` and `errors2`) in the `yt` dataset.*

```{r}
# INSERT CODE HERE
```

*Now create two univariate visualization of these errors. Based on this result, which model looks better? Why?*

```{r}
# INSERT CODE HERE
```

> Write answer here

*Finally, create a multivariate visualization of both sets of errors, comparing them against the $X$ variable. Based on this result, which model looks better? Why?*

```{r}
# INSERT CODE HERE
```

> Write answer here

## Question 8 [2 points]
*Calculate the* **R***oot* **M***ean* **S***quared* **E***rror (RMSE) using 10-fold cross validation with a 50-50 split. How bad are the first model's mistakes on average? How bad are the second model's mistakes? Which model seems better? Remember to talk about the result in terms of the ***univariate** *visualization of the outcome variable!*

```{r,message=F,warning=F}
set.seed(123) # Set the seed to ensure replicability
cvRes <- NULL # Instantiate an empty object to save the results
for(i in 1:10) { # 10-fold cross validation
  # Create the training dataset
  train <- yt %>%
    sample_n(size = round(nrow(.)*.5),
             replace = F)
  
  # Create the testing dataset
  test <- yt %>%
    anti_join(train)
  
  # Estimate the regression on the training dataset
  mTmp <- lm(average_recommendation_ideo ~ average_watch_ideo,
             data = train)
  
  # Predict the model on the testing dataset
  test <- test %>%
    mutate(predY = predict(mTmp,newdata = test))
  
  # Calculate the RMSE
  answer <- test %>%
    mutate(errors = average_recommendation_ideo - predY) %>%
    mutate(se = errors^2) %>%
    summarise(mse = mean(se,na.rm=T)) %>%
    mutate(rmse = sqrt(mse)) %>%
    mutate(cvInd = i)
  
  # Save the result
  cvRes <- cvRes %>%
    bind_rows(answer)
}

# Finally, calculate the RMSE value
mean(cvRes$rmse)
```

> Write answer here

## EC #2 [2 points]

*Let's try including both $X$ variables into a single model. Run the regression and evaluate the errors as described in questions 7. Then evaluate the RMSE using 10-fold cross validation with an 80-20 split. Does this combined model perform better than the two separate models? Worse? Why?*

```{r}
# INSERT CODE HERE
```

> Write answer here.
