<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Classification</title>
    <meta charset="utf-8" />
    <meta name="author" content="Prof. Bisbee" />
    <script src="libs/header-attrs-2.27/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="css/lexis.css" type="text/css" />
    <link rel="stylesheet" href="css/lexis-fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Classification
]
.subtitle[
## Part 3
]
.author[
### Prof. Bisbee
]
.institute[
### Vanderbilt University
]
.date[
### Slides Updated: 2024-07-11
]

---


&lt;style type="text/css"&gt;
.small .remark-code { /*Change made here*/
  font-size: 85% !important;
}
.tiny .remark-code { /*Change made here*/
  font-size: 50% !important;
}
&lt;/style&gt;



# Agenda

1. Recap of regression and classification

2. Introducing (some) machine learning algorithms

---

# What is regression?

--

- Conditional means for continuous data

--


&lt;center&gt;&lt;img src="figs/condmean_reg.png" width = 45%&gt;&lt;/center&gt;

---

# Regression

--

- Calculating a **line** that minimizes mistakes *for every observation*

--

  - NB: could be a curvey line! For now, just assume straight
  
--

- Recall from geometry how to graph a straight line

--

- `\(Y = a + bX\)`

  - `\(a\)`: the "intercept" (where the line intercepts the y-axis)
  - `\(b\)`: the "slope" (how much `\(Y\)` changes for each increase in `\(X\)`)

--

- (Data scientists use `\(\alpha\)` and `\(\beta\)` instead of `\(a\)` and `\(b\)` b/c nerds)

--

- Regression analysis simply chooses the best line

--

  - "Best"?

--

  - The line that minimizes the mistakes (the **line of best fit**)

---

# Visual Intuition

&lt;center&gt;&lt;img src="./figs/raw.png" width = 70%&gt;&lt;/center&gt;


---

# Visual Intuition

&lt;center&gt;&lt;img src="./figs/regression-line.gif" width = 70%&gt;&lt;/center&gt;

---

# Two Camps Revisited

--

- Regression is great for **theory testing**

--

  - Results tell us something **meaningful** about our theory
  
--

- But if all we care about is **prediction**...?

--

  - Want to test every possible predictor (and combinations)
  
  - Don't care about **relationships**
  
  - Just care about **accuracy**
  
--

- Algorithms can save us time!

--

  - Random Forests
  
  - LASSO
  
---

# Random Forests

- Identify the best "partition" (split) that divides the data

--

&lt;center&gt;&lt;img src="./figs/rfdemo.gif" width = 70%&gt;&lt;/center&gt;

--

- In `R`: `ranger`

--

  - `formula = Y ~ .`

---

# Random Forests


``` r
require(tidyverse)
require(scales)
require(tidymodels)
fn &lt;- read_rds('https://github.com/jbisbee1/ISP_Data_Science_2024/raw/main/data/fn_cleaned_final.rds')
```


---

# Research Question

- What predicts whether you win at Fortnite?


``` r
form.perf &lt;- 'won ~ hits + assists + accuracy + head_shots + damage_to_players'

form.games &lt;- 'won ~ eliminations + revives + distance_traveled + materials_gathered'

form.context &lt;- 'won ~ mental_state + startTime + gameIdSession'

form.full &lt;- 'won ~ hits + assists + accuracy + head_shots + damage_to_players + eliminations + revives + distance_traveled + materials_gathered + mental_state + startTime + gameIdSession'
```

---

# Comparing models


``` r
require(broom)
m.perf &lt;- lm(as.formula(form.perf),fn)
tidy(m.perf)
```

```
## # A tibble: 6 × 5
##   term               estimate std.error statistic  p.value
##   &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)        0.0879   0.0377        2.33  1.99e- 2
## 2 hits               0.000696 0.00100       0.695 4.87e- 1
## 3 assists            0.0345   0.0102        3.38  7.64e- 4
## 4 accuracy          -0.416    0.108        -3.85  1.26e- 4
## 5 head_shots        -0.00481  0.00315      -1.53  1.27e- 1
## 6 damage_to_players  0.000473 0.0000571     8.27  4.31e-16
```

---

# Comparing models


``` r
m.games &lt;- lm(as.formula(form.games),fn)
tidy(m.games)
```

```
## # A tibble: 5 × 5
##   term                 estimate std.error statistic  p.value
##   &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)           4.17e-2 0.0221       1.89   5.94e- 2
## 2 eliminations          1.15e-2 0.00976      1.18   2.39e- 1
## 3 revives               6.99e-2 0.0181       3.86   1.19e- 4
## 4 distance_traveled     1.81e-4 0.0000175   10.3    1.31e-23
## 5 materials_gathered   -2.55e-6 0.0000351   -0.0725 9.42e- 1
```

---

# Comparing models


``` r
m.context &lt;- lm(as.formula(form.context),fn)
tidy(m.context)
```

```
## # A tibble: 4 × 5
##   term                  estimate std.error statistic p.value
##   &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
## 1 (Intercept)            9.03e+1   2.99e+1     3.02  2.58e-3
## 2 mental_statesober      1.37e-1   2.93e-2     4.66  3.56e-6
## 3 startTime             -5.67e-8   1.88e-8    -3.02  2.64e-3
## 4 gameIdSession          1.46e-3   1.46e-3     0.998 3.19e-1
```

---

# Evaluate Model Fit


``` r
cvRes &lt;- NULL
for(i in 1:100) {
  inds &lt;- sample(1:nrow(fn),size = round(nrow(fn)*.8),replace = F)
  train &lt;- fn %&gt;% slice(inds)
  test &lt;- fn %&gt;% slice(-inds)
  
  # Train
  mTmp.perf &lt;- lm(as.formula(form.perf),train)
  mTmp.games &lt;- lm(as.formula(form.games),train)
  mTmp.context &lt;- lm(as.formula(form.context),train)
  
  # Test
  toEval &lt;- test %&gt;%
    mutate(prob.p = predict(mTmp.perf,newdata = test),
           prob.g = predict(mTmp.games,newdata = test),
           prob.c = predict(mTmp.context,newdata = test),
           truth = factor(won,levels = c('1','0'))) 
  
  auc.p &lt;- roc_auc(toEval,truth,prob.p) %&gt;%
    mutate(model = 'performance')
  
  auc.g &lt;- roc_auc(toEval,truth,prob.g) %&gt;%
    mutate(model = 'games')
  
  auc.c &lt;- roc_auc(toEval,truth,prob.c) %&gt;%
    mutate(model = 'context')
    
  cvRes &lt;- cvRes %&gt;%
    bind_rows(auc.p) %&gt;%
    bind_rows(auc.g) %&gt;%
    bind_rows(auc.c) %&gt;%
    mutate(cvInd = i)
}
```

---

# Evaluate Model Fit


``` r
cvRes %&gt;%
  ggplot(aes(x = .estimate,y = model)) + 
  geom_boxplot() + labs(x = 'AUC',y = 'Specification')
```

&lt;img src="ISP_lecture_10_slides_files/figure-html/unnamed-chunk-9-1.png" style="display: block; margin: auto;" /&gt;

---

# Random Forests


``` r
require(ranger) # Fast random forests package
rf.f &lt;- ranger(formula = as.formula(form.full),data = fn)

toEval &lt;- fn %&gt;%
  mutate(prob_won = rf.f$predictions) %&gt;%
  mutate(truth = factor(won,levels = c('1','0')))

roc_auc(toEval,truth,prob_won)
```

```
## # A tibble: 1 × 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 roc_auc binary         0.837
```

---

# Random Forest Comparison


``` r
cvRes &lt;- NULL
for(i in 1:100) {
  inds &lt;- sample(1:nrow(fn),size = round(nrow(fn)*.8),replace = F)
  train &lt;- fn %&gt;% slice(inds)
  test &lt;- fn %&gt;% slice(-inds)
  
  # Train
  mLM.f &lt;- lm(as.formula(form.full),train)
  mRF.f &lt;- ranger(as.formula(form.full),train)
  
  # Test
  # NEED TO RUN PREDICTION ON RF FIRST
  tmpPred &lt;- predict(mRF.f,test)
  
  toEval &lt;- test %&gt;%
    mutate(prob.lm = predict(mLM.f,newdata = test),
           prob.rf = tmpPred$predictions,
           truth = factor(won,levels = c('1','0')))
  
  auc.lm &lt;- roc_auc(toEval,truth,prob.lm) %&gt;%
    mutate(model = 'linear')
  
  auc.rf &lt;- roc_auc(toEval,truth,prob.rf) %&gt;%
    mutate(model = 'random forest')

    cvRes &lt;- cvRes %&gt;%
    bind_rows(auc.lm) %&gt;%
    bind_rows(auc.rf) %&gt;%
    mutate(cvInd = i)
}
```

---

# Random Forest Comparison


``` r
cvRes %&gt;%
  ggplot(aes(x = .estimate,y = model)) + 
  geom_boxplot() + labs(x = 'AUC',y = 'Algorithm')
```

&lt;img src="ISP_lecture_10_slides_files/figure-html/unnamed-chunk-12-1.png" style="display: block; margin: auto;" /&gt;

---

# What matters most?

- Random Forests are particularly suitable for investigating **variable importance**

--

  - I.e., which `\(X\)` predictors are most helpful?
  
--

- A few options, but we rely on **permutation tests**

--

  - Idea: run the best model you have, then re-run it after "permuting" one of the variables
  
  - "Permute" means randomly reshuffle...breaks relationship
  
  - How much **worse** is the model when you break a variable?
  
---

# Variable Importance

- In `ranger()`, use `importance = "permutation"`


``` r
rf.full &lt;- ranger(formula = as.formula(form.full),data = fn %&gt;%
                    mutate(mental_state = ifelse(mental_state == 'sober',1,0)),importance = 'permutation')

rf.full$variable.importance
```

```
##               hits            assists           accuracy 
##        0.013803552        0.003892707        0.005061289 
##         head_shots  damage_to_players       eliminations 
##        0.005871168        0.033652154        0.011048596 
##            revives  distance_traveled materials_gathered 
##        0.005001197        0.068280506        0.026299848 
##       mental_state          startTime      gameIdSession 
##        0.005080621        0.000381819        0.027336278
```

---

# Variable Importance

.small[

``` r
toplot &lt;- data.frame(vimp = rf.full$variable.importance,
                     vars = names(rf.full$variable.importance))

toplot %&gt;%
  ggplot(aes(x = vimp,y = reorder(vars,vimp))) + 
  geom_bar(stat = 'identity') + labs(x = 'VIMP',y = 'Predictor')
```

&lt;img src="ISP_lecture_10_slides_files/figure-html/unnamed-chunk-14-1.png" style="display: block; margin: auto;" /&gt;
]


---

# LASSO

--

- "Least Absolute Shrinkage and Selection Operator"

--

- Concept: Make it hard for predictors to matter

--

  - Practice: `\(\lambda\)` penalizes how many variables you can include
  
  - `\(\sum_{i = 1}^n (y_i - \sum_j x_{ij}\beta_j)^2 + \lambda \sum_{j=1}^p |\beta_j|\)`
  
  - Minimize the errors, but penalize for each additional predictor
  
  - You *could* kitchen-sink a regression and get super low errors
  
  - LASSO penalizes you from throwing everything into the kitchen sink

--

- In `R`, need to install a new package! `install.packages('glmnet')`


``` r
require(glmnet)
```

---

# LASSO

- Function doesn't use formulas

--

- Give it the raw data instead, divided into `Y` (outcome) and `X` (predictors)

--


``` r
rhsVars &lt;- str_split(gsub('won ~ ','',form.full),' \\+ ')[[1]]
fn &lt;- fn %&gt;%
  drop_na(all_of(rhsVars))
Y &lt;- fn$won
X &lt;- fn %&gt;% select(all_of(rhsVars)) %&gt;%
  mutate(mental_state = ifelse(mental_state == 'sober',1,0),
         startTime = as.numeric(startTime))
```


---

# LASSO

- Now estimate!


``` r
lassFit &lt;- glmnet(x = as.matrix(X),
                  y = as.matrix(Y))
```

---

# LASSO


``` r
plot(lassFit)
```

&lt;img src="ISP_lecture_10_slides_files/figure-html/unnamed-chunk-18-1.png" style="display: block; margin: auto;" /&gt;

---

# Has its own CV!


``` r
cv.lassFit &lt;- cv.glmnet(x = as.matrix(X),y = as.matrix(Y))

plot(cv.lassFit)
```

&lt;img src="ISP_lecture_10_slides_files/figure-html/unnamed-chunk-19-1.png" style="display: block; margin: auto;" /&gt;

---

# Variable Importance


``` r
best &lt;- cv.lassFit$glmnet.fit$beta[,cv.lassFit$index[2,]]
vimpLass &lt;- data.frame(vimp = best,
                       vars = names(best))
vimpLass %&gt;%
  ggplot(aes(x = vimp,y = reorder(vars,vimp))) + 
  geom_bar(stat = 'identity')
```

&lt;img src="ISP_lecture_10_slides_files/figure-html/unnamed-chunk-20-1.png" style="display: block; margin: auto;" /&gt;

---

# Conclusion

- Lots of powerful tools out there!

- Make sure to take **more classes** on these topics!

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="libs/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
